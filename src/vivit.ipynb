{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c26d1b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b427ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:09:58.024201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "SEED = 42\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c38e8",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99988d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (28, 28, 28, 1)\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7d591",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a59ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MD5': '21f0a239e7f502e6eca33c3fc453c0b6',\n",
      " 'description': 'The source of the OrganMNIST3D is the same as that of the '\n",
      "                'Organ{A,C,S}MNIST. Instead of 2D images, we directly use the '\n",
      "                '3D bounding boxes and process the images into 28×28×28 to '\n",
      "                'perform multi-class classification of 11 body organs. The '\n",
      "                'same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the '\n",
      "                'source training set are used as training and validation set, '\n",
      "                'respectively, and the same 70 CT scans as the '\n",
      "                'Organ{A,C,S}MNIST from the source test set are treated as the '\n",
      "                'test set.',\n",
      " 'label': {'0': 'liver',\n",
      "           '1': 'kidney-right',\n",
      "           '10': 'pancreas',\n",
      "           '2': 'kidney-left',\n",
      "           '3': 'femur-right',\n",
      "           '4': 'femur-left',\n",
      "           '5': 'bladder',\n",
      "           '6': 'heart',\n",
      "           '7': 'lung-right',\n",
      "           '8': 'lung-left',\n",
      "           '9': 'spleen'},\n",
      " 'license': 'CC BY 4.0',\n",
      " 'n_channels': 1,\n",
      " 'n_samples': {'test': 610, 'train': 972, 'val': 161},\n",
      " 'python_class': 'OrganMNIST3D',\n",
      " 'task': 'multi-class',\n",
      " 'url': 'https://zenodo.org/record/6496656/files/organmnist3d.npz?download=1'}\n"
     ]
    }
   ],
   "source": [
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    \"\"\"Utility function to download the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata.\n",
    "    \"\"\"\n",
    "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get videos\n",
    "        train_videos = data[\"train_images\"]\n",
    "        valid_videos = data[\"val_images\"]\n",
    "        test_videos = data[\"test_images\"]\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data[\"train_labels\"].flatten()\n",
    "        valid_labels = data[\"val_labels\"].flatten()\n",
    "        test_labels = data[\"test_labels\"].flatten()\n",
    "\n",
    "    return (\n",
    "        (train_videos, train_labels),\n",
    "        (valid_videos, valid_labels),\n",
    "        (test_videos, test_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "# Get the metadata of the dataset\n",
    "info = medmnist.INFO[DATASET_NAME]\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(info)\n",
    "\n",
    "# Get the dataset\n",
    "prepared_dataset = download_and_prepare_dataset(info)\n",
    "(train_videos, train_labels) = prepared_dataset[0]\n",
    "(valid_videos, valid_labels) = prepared_dataset[1]\n",
    "(test_videos, test_labels) = prepared_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155e13f",
   "metadata": {},
   "source": [
    "# tf.data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1d96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b047032",
   "metadata": {},
   "source": [
    "# Tubelet Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c627ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e0eba",
   "metadata": {},
   "source": [
    "# Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c4d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc305a9",
   "metadata": {},
   "source": [
    "# Video Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b91ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57dd8f4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80358ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:10:24.102347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [972]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-08 20:10:24.107834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype uint8 and shape [972,28,28,28]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 2.5485 - accuracy: 0.1111 - top-5-accuracy: 0.5597"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:11:22.924173: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [161]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-08 20:11:22.942282: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [161]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 73s 1s/step - loss: 2.5485 - accuracy: 0.1111 - top-5-accuracy: 0.5597 - val_loss: 2.3513 - val_accuracy: 0.1988 - val_top-5-accuracy: 0.5776\n",
      "Epoch 2/60\n",
      "31/31 [==============================] - 30s 972ms/step - loss: 2.2323 - accuracy: 0.1800 - top-5-accuracy: 0.6687 - val_loss: 2.0809 - val_accuracy: 0.2174 - val_top-5-accuracy: 0.7205\n",
      "Epoch 3/60\n",
      "31/31 [==============================] - 22s 710ms/step - loss: 2.0593 - accuracy: 0.2150 - top-5-accuracy: 0.7767 - val_loss: 1.8950 - val_accuracy: 0.3106 - val_top-5-accuracy: 0.8075\n",
      "Epoch 4/60\n",
      "31/31 [==============================] - 23s 734ms/step - loss: 1.9996 - accuracy: 0.2263 - top-5-accuracy: 0.7973 - val_loss: 1.7612 - val_accuracy: 0.3230 - val_top-5-accuracy: 0.8509\n",
      "Epoch 5/60\n",
      "31/31 [==============================] - 24s 771ms/step - loss: 1.8056 - accuracy: 0.3107 - top-5-accuracy: 0.8663 - val_loss: 1.7071 - val_accuracy: 0.4099 - val_top-5-accuracy: 0.8447\n",
      "Epoch 6/60\n",
      "31/31 [==============================] - 23s 754ms/step - loss: 1.6118 - accuracy: 0.4023 - top-5-accuracy: 0.8899 - val_loss: 1.2863 - val_accuracy: 0.4596 - val_top-5-accuracy: 0.9503\n",
      "Epoch 7/60\n",
      "31/31 [==============================] - 25s 795ms/step - loss: 1.4305 - accuracy: 0.4362 - top-5-accuracy: 0.9424 - val_loss: 1.3678 - val_accuracy: 0.4783 - val_top-5-accuracy: 0.9503\n",
      "Epoch 8/60\n",
      "31/31 [==============================] - 23s 738ms/step - loss: 1.3613 - accuracy: 0.4825 - top-5-accuracy: 0.9434 - val_loss: 1.0644 - val_accuracy: 0.5466 - val_top-5-accuracy: 0.9814\n",
      "Epoch 9/60\n",
      "31/31 [==============================] - 23s 745ms/step - loss: 1.2330 - accuracy: 0.5185 - top-5-accuracy: 0.9527 - val_loss: 1.0638 - val_accuracy: 0.5404 - val_top-5-accuracy: 0.9752\n",
      "Epoch 10/60\n",
      "31/31 [==============================] - 23s 738ms/step - loss: 1.2077 - accuracy: 0.5288 - top-5-accuracy: 0.9465 - val_loss: 1.0038 - val_accuracy: 0.5590 - val_top-5-accuracy: 0.9752\n",
      "Epoch 11/60\n",
      "31/31 [==============================] - 22s 715ms/step - loss: 1.1370 - accuracy: 0.5556 - top-5-accuracy: 0.9578 - val_loss: 0.8712 - val_accuracy: 0.6708 - val_top-5-accuracy: 0.9876\n",
      "Epoch 12/60\n",
      "31/31 [==============================] - 23s 741ms/step - loss: 0.9875 - accuracy: 0.6327 - top-5-accuracy: 0.9743 - val_loss: 0.8267 - val_accuracy: 0.6894 - val_top-5-accuracy: 0.9814\n",
      "Epoch 13/60\n",
      "31/31 [==============================] - 22s 721ms/step - loss: 0.8961 - accuracy: 0.6790 - top-5-accuracy: 0.9774 - val_loss: 1.0101 - val_accuracy: 0.5963 - val_top-5-accuracy: 0.9752\n",
      "Epoch 14/60\n",
      "31/31 [==============================] - 23s 739ms/step - loss: 0.9447 - accuracy: 0.6502 - top-5-accuracy: 0.9794 - val_loss: 0.6952 - val_accuracy: 0.7578 - val_top-5-accuracy: 0.9876\n",
      "Epoch 15/60\n",
      "31/31 [==============================] - 23s 734ms/step - loss: 0.7505 - accuracy: 0.7160 - top-5-accuracy: 0.9907 - val_loss: 0.6066 - val_accuracy: 0.8012 - val_top-5-accuracy: 0.9938\n",
      "Epoch 16/60\n",
      "31/31 [==============================] - 22s 711ms/step - loss: 0.7366 - accuracy: 0.7366 - top-5-accuracy: 0.9887 - val_loss: 0.6133 - val_accuracy: 0.7950 - val_top-5-accuracy: 0.9938\n",
      "Epoch 17/60\n",
      "31/31 [==============================] - 21s 668ms/step - loss: 0.6445 - accuracy: 0.7613 - top-5-accuracy: 0.9907 - val_loss: 0.5820 - val_accuracy: 0.7950 - val_top-5-accuracy: 0.9938\n",
      "Epoch 18/60\n",
      "31/31 [==============================] - 21s 676ms/step - loss: 0.5501 - accuracy: 0.8076 - top-5-accuracy: 0.9907 - val_loss: 0.4619 - val_accuracy: 0.8261 - val_top-5-accuracy: 0.9938\n",
      "Epoch 19/60\n",
      "31/31 [==============================] - 19s 600ms/step - loss: 0.5587 - accuracy: 0.7984 - top-5-accuracy: 0.9928 - val_loss: 0.8126 - val_accuracy: 0.7391 - val_top-5-accuracy: 0.9876\n",
      "Epoch 20/60\n",
      "31/31 [==============================] - 19s 610ms/step - loss: 0.6097 - accuracy: 0.7850 - top-5-accuracy: 0.9928 - val_loss: 0.5618 - val_accuracy: 0.8012 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "31/31 [==============================] - 21s 672ms/step - loss: 0.6135 - accuracy: 0.7716 - top-5-accuracy: 0.9938 - val_loss: 0.5461 - val_accuracy: 0.7640 - val_top-5-accuracy: 0.9938\n",
      "Epoch 22/60\n",
      "31/31 [==============================] - 21s 683ms/step - loss: 0.4616 - accuracy: 0.8333 - top-5-accuracy: 0.9979 - val_loss: 0.4503 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9938\n",
      "Epoch 23/60\n",
      "31/31 [==============================] - 23s 741ms/step - loss: 0.3914 - accuracy: 0.8632 - top-5-accuracy: 0.9959 - val_loss: 0.3762 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9876\n",
      "Epoch 24/60\n",
      "31/31 [==============================] - 20s 655ms/step - loss: 0.4365 - accuracy: 0.8477 - top-5-accuracy: 0.9938 - val_loss: 0.4120 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9876\n",
      "Epoch 25/60\n",
      "31/31 [==============================] - 24s 786ms/step - loss: 0.3446 - accuracy: 0.8879 - top-5-accuracy: 0.9979 - val_loss: 0.4606 - val_accuracy: 0.8199 - val_top-5-accuracy: 0.9814\n",
      "Epoch 26/60\n",
      "31/31 [==============================] - 22s 719ms/step - loss: 0.3336 - accuracy: 0.8693 - top-5-accuracy: 0.9990 - val_loss: 0.3670 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9814\n",
      "Epoch 27/60\n",
      "31/31 [==============================] - 22s 705ms/step - loss: 0.2593 - accuracy: 0.9095 - top-5-accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 28/60\n",
      "31/31 [==============================] - 21s 664ms/step - loss: 0.2566 - accuracy: 0.9156 - top-5-accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/60\n",
      "31/31 [==============================] - 21s 668ms/step - loss: 0.2562 - accuracy: 0.9064 - top-5-accuracy: 0.9990 - val_loss: 0.5024 - val_accuracy: 0.8385 - val_top-5-accuracy: 0.9876\n",
      "Epoch 30/60\n",
      "31/31 [==============================] - 21s 656ms/step - loss: 0.2920 - accuracy: 0.8971 - top-5-accuracy: 1.0000 - val_loss: 0.4325 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9876\n",
      "Epoch 31/60\n",
      "31/31 [==============================] - 17s 527ms/step - loss: 0.1920 - accuracy: 0.9362 - top-5-accuracy: 1.0000 - val_loss: 0.3037 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/60\n",
      "31/31 [==============================] - 20s 631ms/step - loss: 0.1517 - accuracy: 0.9444 - top-5-accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
      "Epoch 33/60\n",
      "31/31 [==============================] - 23s 729ms/step - loss: 0.1296 - accuracy: 0.9609 - top-5-accuracy: 1.0000 - val_loss: 0.3630 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
      "Epoch 34/60\n",
      "31/31 [==============================] - 21s 684ms/step - loss: 0.1266 - accuracy: 0.9568 - top-5-accuracy: 1.0000 - val_loss: 0.4147 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 35/60\n",
      "31/31 [==============================] - 24s 775ms/step - loss: 0.1505 - accuracy: 0.9496 - top-5-accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.8447 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/60\n",
      "31/31 [==============================] - 18s 589ms/step - loss: 0.1205 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 37/60\n",
      "31/31 [==============================] - 17s 565ms/step - loss: 0.1238 - accuracy: 0.9578 - top-5-accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.8758 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/60\n",
      "31/31 [==============================] - 17s 559ms/step - loss: 0.1648 - accuracy: 0.9414 - top-5-accuracy: 1.0000 - val_loss: 0.4458 - val_accuracy: 0.8758 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/60\n",
      "31/31 [==============================] - 20s 636ms/step - loss: 0.1139 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.3886 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 40/60\n",
      "31/31 [==============================] - 20s 646ms/step - loss: 0.0575 - accuracy: 0.9846 - top-5-accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 41/60\n",
      "31/31 [==============================] - 19s 600ms/step - loss: 0.0460 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9876\n",
      "Epoch 42/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 20s 645ms/step - loss: 0.0382 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 43/60\n",
      "31/31 [==============================] - 18s 570ms/step - loss: 0.0613 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.4556 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 44/60\n",
      "31/31 [==============================] - 19s 609ms/step - loss: 0.0496 - accuracy: 0.9856 - top-5-accuracy: 1.0000 - val_loss: 0.4232 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/60\n",
      "31/31 [==============================] - 19s 610ms/step - loss: 0.0234 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.4380 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 46/60\n",
      "31/31 [==============================] - 19s 606ms/step - loss: 0.0351 - accuracy: 0.9877 - top-5-accuracy: 1.0000 - val_loss: 0.4302 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9938\n",
      "Epoch 47/60\n",
      "31/31 [==============================] - 19s 626ms/step - loss: 0.1346 - accuracy: 0.9568 - top-5-accuracy: 0.9990 - val_loss: 0.5199 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9938\n",
      "Epoch 48/60\n",
      "31/31 [==============================] - 19s 609ms/step - loss: 0.1237 - accuracy: 0.9568 - top-5-accuracy: 1.0000 - val_loss: 0.4857 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9814\n",
      "Epoch 49/60\n",
      "31/31 [==============================] - 18s 592ms/step - loss: 0.0415 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 50/60\n",
      "31/31 [==============================] - 19s 599ms/step - loss: 0.0329 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 51/60\n",
      "31/31 [==============================] - 19s 602ms/step - loss: 0.0244 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/60\n",
      "31/31 [==============================] - 18s 577ms/step - loss: 0.0170 - accuracy: 0.9959 - top-5-accuracy: 1.0000 - val_loss: 0.4249 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
      "Epoch 53/60\n",
      "31/31 [==============================] - 19s 626ms/step - loss: 0.0062 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 54/60\n",
      "31/31 [==============================] - 19s 603ms/step - loss: 0.0060 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 55/60\n",
      "31/31 [==============================] - 18s 579ms/step - loss: 0.0042 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3848 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 56/60\n",
      "31/31 [==============================] - 18s 576ms/step - loss: 0.0032 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
      "Epoch 57/60\n",
      "31/31 [==============================] - 23s 732ms/step - loss: 0.0027 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 58/60\n",
      "31/31 [==============================] - 19s 613ms/step - loss: 0.0024 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3987 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 59/60\n",
      "31/31 [==============================] - 18s 598ms/step - loss: 0.0021 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3997 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 60/60\n",
      "31/31 [==============================] - 16s 519ms/step - loss: 0.0021 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3939 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:42:39.305729: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [610]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-08 20:42:39.313232: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [610]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 3s 151ms/step - loss: 1.0551 - accuracy: 0.7852 - top-5-accuracy: 0.9721\n",
      "Test accuracy: 78.52%\n",
      "Test top 5 accuracy: 97.21%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35114c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5704eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "19/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07395a4c01244635ac081f328f3697a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: pancreas | P: pancreas'\"), Box(children=(Image(value=b'GIF89a…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 25\n",
    "testsamples, labels = next(iter(testloader))\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy().squeeze() * 255).astype(\"uint8\"), \"GIF\", **{'duration': 0.1*1000})\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "truec = 0\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = info[\"label\"][str(ground_truths[i])]\n",
    "    pred_class = info[\"label\"][str(preds[i])]\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "    \n",
    "    if true_class == pred_class :\n",
    "        truec += 1\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "print(f\"{truec}/{NUM_SAMPLES_VIZ}\")\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacf1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
